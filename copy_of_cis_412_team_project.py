# -*- coding: utf-8 -*-
"""Copy of CIS 412 Team Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ViylIOADPsHLZOdIuvcxfbtMIrQZCCo4
"""

# SOLVING THE BUSINESS PROBELM THROUGH LOGISTIC REGRESSION AND DECISION TREE

# This project aims to identify the key factors that influence passenger satisfaction and develop a
# model to predict whether a passenger is likely to be satisfied or dissatisfied based on survey data.

# Import the data set
import pandas as pd
import numpy as np
df = pd.read_csv("/content/test.csv")

# View first few rows
df.head()

# What is unnamed 0? I don't think we need it. I also don't think id is helpful either so lets drop them?

# Dropping the 2 uneccessary columns.
df.drop(['Unnamed: 0', 'id'], axis=1, inplace=True)

#Check that they got dropped
df.columns[:10]
df.head()

# Check missing values
df.isnull().sum()

# What I did here is fill in all the missing values with median of arrival delay in minutes
df['Arrival Delay in Minutes'] = df['Arrival Delay in Minutes'].fillna(df['Arrival Delay in Minutes'].median())

# Check that missing values are handled
df['Arrival Delay in Minutes'].isnull().sum()

# Now let's look at our data types
df.info()

# We have 5 categorical (object) columns...17 numeric (int)...1 floating(float64)
# Machine learning models can’t handle text — they can only work with numbers.
# So we’ll convert those 5 columns into numeric format.

# But we’ll do it in two parts:
# 1. Convert the target column (satisfaction) to binary numbers.
# 2. Encode the remaining categorical columns (the features).

# 1. Convert the target column (satisfaction) to binary numbers.
df['satisfaction'].value_counts()

# Now convert it into numbers
df['satisfaction'] = df['satisfaction'].map({
    'satisfied': 1,
    'neutral or dissatisfied': 0,})

# Check
df['satisfaction'].unique()

# Now let's encode the other features
from sklearn.preprocessing import LabelEncoder

label_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']
le = LabelEncoder()

for col in label_cols:
    df[col] = le.fit_transform(df[col])

df.head()

# Check
df.info()

import seaborn as sns
import matplotlib.pyplot as plt

# Plot satisfaction count
sns.countplot(data=df, x='satisfaction')
plt.title('Satisfaction Count')
plt.show()

# Check for duplicate rows
print("Duplicate rows:", df.duplicated().sum())

# Check possible outliers
num_cols = ['Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']
for col in num_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower) | (df[col] > upper)]
    pct = (len(outliers) / len(df)) * 100
    print(f"{col}: {len(outliers)} outliers ({pct:.2f}%)")

    # Keep all the outliers because maybe ...
    # 1. Flight distance: Some flights may be very short and some very long
    # 2. Departure Delay: Some flights delayed much longer than usual
    # 3. Same as above — large but realistic delays

# Creating a heatmap to look at correlations
# When two features are highly correlated (e.g., Departure Delay and Arrival Delay)
# they contain almost the same information.
# Including both in some models (like Logistic Regression) can:
# 1. cause multicollinearity (unstable coefficients),
# 2. make the model more complex than necessary,
# 3. slightly reduce interpretability.
# So, plotting a correlation heatmap to visually check which numeric columns move together.

import seaborn as sns
import matplotlib.pyplot as plt

# Compute correlation matrix
correlation = df.corr()

# Plot heatmap
plt.figure(figsize=(14,8))
sns.heatmap(
    correlation,
    annot=True,            # show correlation values
    fmt='.2f',             # limit decimals
    annot_kws={'size': 8}, # smaller annotation text
    linewidths=0.5,
    cmap='coolwarm'        # color scheme
)
plt.title("Feature Correlation Heatmap", fontsize=14)
plt.show()

# We see that Departure Delay in Minutes ↔ Arrival Delay in Minutes → 0.96 correlation
# They’re basically measuring the same thing.

# Can safely drop one of them
# Code to keep just Arrival Delay in Minutes, since it represents the final delay outcome.
# Below is code to drop "Departure Delay in Minutes"

df.drop('Departure Delay in Minutes', axis=1, inplace=True)

# -------------------------- DONE CLEANING AND PREPARING THE DATA --------------------------------------------

from sklearn.model_selection import train_test_split

# Separate features (X) and target (y)
X = df.drop('satisfaction', axis=1)
y = df['satisfaction']

# Split into 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

# LET'S GO AHEAD WITH LOGISTIC REGRESSION TESTING FIRST
# ---------------------------
# Logistic Regression: scale, fit, evaluate, visualize, save
# ---------------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_auc_score, roc_curve
)
import joblib

# 1) Prepare copies for logistic regression (so tree model can use raw data)
X_train_lr = X_train.copy()
X_test_lr  = X_test.copy()

# 2) Fit scaler only on training data — prevent data leakage
scaler = StandardScaler()
X_train_lr_scaled = scaler.fit_transform(X_train_lr)
X_test_lr_scaled  = scaler.transform(X_test_lr)

# 3) Train logistic regression
lr = LogisticRegression(max_iter=2000, random_state=42)
lr.fit(X_train_lr_scaled, y_train)

# 4) Predict and evaluate
y_pred = lr.predict(X_test_lr_scaled)
y_proba = lr.predict_proba(X_test_lr_scaled)[:, 1]  # prob for positive class (satisfied)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ROC AUC
try:
    auc = roc_auc_score(y_test, y_proba)
    print("ROC AUC:", round(auc, 4))
except Exception as e:
    print("ROC AUC could not be computed:", e)

# 5) Confusion matrix (colored)
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1], yticklabels=[0,1])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# 6) ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc_score(y_test, y_proba):.3f}')
plt.plot([0,1],[0,1],'--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# 7) Coefficients interpretation (feature -> coef)
# If X_train is a DataFrame, get names; otherwise build a list
if hasattr(X_train, "columns"):
    feature_names = X_train.columns.tolist()
else:
    feature_names = [f"f{i}" for i in range(X_train.shape[1])]

coefs = lr.coef_.ravel()
coef_df = pd.DataFrame({'feature': feature_names, 'coef': coefs})
coef_df['abs_coef'] = coef_df['coef'].abs()
coef_df = coef_df.sort_values('abs_coef', ascending=False)

print("\nTop features by absolute coefficient (positive increases prob of satisfaction):")
display(coef_df.head(12)[['feature','coef']])

# In the confusion matrix ( 0 = dissatisfied and 1 = satisfied )
# According to our matrix .. We have a strong balance — both classes are predicted well.
# AUC of 0.926 means your model can correctly distinguish satisfied/dissatisfied passengers ~ 93% of the time
# This is excellent performance for logistic regression.

# Most influential positive factors: Online boarding, Wi-Fi, Check-in service.
# Most influential negative factors: Type of Travel, Customer Type, Delay times.

# LET'S NOW GO TO THE DECISION TREE TESTING
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train Decision Tree
dt_model = DecisionTreeClassifier(
    criterion='entropy',     # or 'entropy' if you prefer information gain
    max_depth=6,          # limit depth to prevent overfitting
    random_state=42
)
dt_model.fit(X_train, y_train)

# Predictions
y_pred_dt = dt_model.predict(X_test)

# Evaluate performance
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print("\nClassification Report:\n", classification_report(y_test, y_pred_dt))

# Confusion Matrix
cm_dt = confusion_matrix(y_test, y_pred_dt)
plt.figure(figsize=(5,4))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Greens', xticklabels=[0,1], yticklabels=[0,1])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Decision Tree Confusion Matrix')
plt.show()

# Feature Importances
importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': dt_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nTop Features by Importance:")
display(importances.head(10))

# Optional: Visualize the tree (for slides)
plt.figure(figsize=(18,10))
plot_tree(dt_model, filled=True, feature_names=X_train.columns, class_names=['Dissatisfied', 'Satisfied'], rounded=True, fontsize=8)
plt.title("Decision Tree Visualization (max_depth=6)")
plt.show()

# INSIGHTS

# Decision Tree performed slightly better — it captures nonlinear relationships the logistic model can’t.
#It shows Online boarding and Inflight wifi are the biggest drivers of satisfaction.

# Both models are consistent — confirming your data is clean and strong.

# Logistic Regression, while slightly less accurate, is more interpretable
# Great for explaining which features increase or decrease satisfaction.

# Why the Decision Tree performs better

# 1. Captures nonlinear relationships:
# Logistic Regression assumes a straight-line (linear) relationship between features and the outcome.
# But passenger satisfaction depends on combinations of factors (like Wi-Fi + boarding + travel type).
# Decision Trees can easily model these complex, “if–then” patterns.

# 2. Handles interactions automatically:
# Example: If Type of Travel = Business and Online boarding rating ≥ 4, then high satisfaction.
# Logistic Regression can’t capture such conditional relationships unless you manually create interaction terms.

# 3. Insensitive to feature scaling:
# Trees don’t need normalization — they split directly based on feature values, making them robust when different features have different ranges.

# 4. Good for mixed data (categorical + numeric):
# Your dataset has both types of variables, and trees handle them naturally.